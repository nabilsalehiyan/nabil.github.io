---
title: "Linear Discriminant Analysis"
author: "Nabil Salehiyan"
---

```{r}
# Knowledge Mining: Linear Discriminant Analysis
# File: Lab_LDA01.R
# Theme: Linear Discriminant Analysis
# Adapted from ISLR Chapter 4 Lab

# Setup
require(ISLR)
require(MASS)
require(descr)
attach(Smarket)

## Linear Discriminant Analysis
freq(Direction)
train = Year<2005
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
lda.fit
plot(lda.fit, col="dodgerblue")
Smarket.2005=subset(Smarket,Year==2005) # Creating subset with 2005 data for prediction
lda.pred=predict(lda.fit,Smarket.2005)
names(lda.pred)
lda.class=lda.pred$class
Direction.2005=Smarket$Direction[!train] 
table(lda.class,Direction.2005) 
data.frame(lda.pred)[1:5,]
table(lda.pred$class,Smarket.2005$Direction)
mean(lda.pred$class==Smarket.2005$Direction)
```

From the three methods (best subset, forward stepwise, and backward stepwise):

a.  Which of the three models with k predictors has the smallest training RSS?

From these methods, best subset selects the model with the smallest training RSS. This method searches through all the subsets of predictors and picks the one with the lowest RSS on the training data.

b\. Which of the three models with k predictors has the smallest test RSS?

In order to pick the model with the smallest test RSS, we need to estimate the error. We can do this using validation or cross-validation, or indirectly estimate the error by making an adjustment to the training error to account for the bias that comes from overfitting It also depends on other factors such as number of variables and curve of the data. Cp, AIC, BIC, and Adjusted R2 are all techniques for adjusting the training error and allows us to select amongst models with different numbers of variables.

Cp: This statistic takes on a small value when the test error is also small.

AIC: This statistic is best for a large class of models fit by maximum likelihood

BIC: Dervied from a bayesian perspective. This also takes on a smaller value as the test error decreases. This method adds a heavier penalty on models with many variables which results in smaller model selection compared to Cp.

R\^2: this statistic increases as test error decreases.

## Application Exercise

```{r}
#Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector ðœ€ of length n = 100.
set.seed(1)
X = rnorm(100)
eps = rnorm(100)

# Generate response vector according to model and plot x and y
y <- 4 + 9*X + 2*X^2 + X^3 + eps
plot(X, y, main="Response vs Predictor X", xlab="X", ylab="Response")


```

```{r}
# Load leaps package and perform best subset selection
require(leaps)
library(leaps)
df <- data.frame(X, y)
subsets <- regsubsets(y ~ poly(X, 10, raw=T), data=df, nvmax=10)
```

```{r}
# Plot RSS, Cp, BIC, and adjusted R^2 for each model size
plot(subsets, scale = "Cp")
plot(subsets, scale = "bic")
plot(subsets, scale = "adjr2")
```

```{r}
# Get best model according to Cp
summary(subsets)$which[which.min(summary(subsets)$cp),]

# Get best model according to BIC
summary(subsets)$which[which.min(summary(subsets)$bic),]

# Get best model according to adjusted R^2
summary(subsets)$which[which.max(summary(subsets)$rsq),]

summary(subsets)
```

```{r}
coef(subsets, 10)
```

In this we see that the best models have the smallest coefs, signifying a smaller RSS value and larger R squared. We can see that as the variables increase in number, the best model also changes.

5\. Repeat 3, using forward stepwise selection and using backwards stepwise selection. How does your answer compare to the results in 3?

```{r}
# Forward stepwise selection
fit_fwd <- regsubsets(y ~ poly(X, 10, raw = TRUE), data = data.frame(y, X), nvmax = 10, method = "forward")
summary(fit_fwd)

# Backward stepwise selection
fit_bwd <- regsubsets(y ~ poly(X, 10, raw = TRUE), data = data.frame(y, X), nvmax = 10, method = "backward")
summary(fit_bwd)

```

```{r}
par(mfrow=c(1,2))  
plot(fit_fwd, which="fit", main="Forward Selection")  
plot(fit_bwd, which="fit", main="Backward Elimination")  
```

```{r}
coef(fit_fwd, 10)
```

```{r}
coef(fit_bwd, 10)
```

Here we see that when using forward stepwise and backwards stepwise slection, we get the same coefs as the Cp, BIC, and R2. This makes sense because we have 10 predictors and the data doesnt follow the conditions that lead to the phenomenon mentioned earlier. These methods can also be more computationally efficient.
