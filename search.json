[
  {
    "objectID": "Lab02.html",
    "href": "Lab02.html",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A)\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\")\n# fix(Auto) # Starting the X11 R data editor\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\",header=T,na.strings=\"?\")\n# fix(Auto)\nAuto=read.csv(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.csv\",header=T,na.strings=\"?\")\n# fix(Auto)\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,]\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/fg/l9nwkw5954b_7xv794xmt5_40000gn/T//RtmporgFfq/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  < 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "Lab01.html",
    "href": "Lab01.html",
    "title": "EPPS 6302: Lab01",
    "section": "",
    "text": "x <- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=TRUE) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.996384\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y, pch=20, col = \"blue\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"blue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "library(haven) \nTEDS_2016 <-\n  read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n#TEDS_2016\nTEDS_2016$PartyID <- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\nattach(TEDS_2016)\nhead(PartyID)\n\n[1] NA  NA  KMT NA  NA  DPP\nLevels: KMT DPP NP PFP TSU NPP NA\n\ntail(PartyID)\n\n[1] NA  NA  DPP NA  NA  NA \nLevels: KMT DPP NP PFP TSU NPP NA\n\ninstall.packages(\"descr\", repos = \"http://cran.us.r-project.org\")\n\n\n  There is a binary version available but the source version is later:\n      binary source needs_compilation\ndescr  1.1.5  1.1.6              TRUE\n\n\ninstalling the source package 'descr'\n\nlibrary(descr)\nfreq(TEDS_2016$PartyID)\n\nTEDS_2016$PartyID \n      Frequency  Percent\nKMT         388  22.9586\nDPP         591  34.9704\nNP            3   0.1775\nPFP          32   1.8935\nTSU           5   0.2959\nNPP          43   2.5444\nNA          628  37.1598\nTotal      1690 100.0000\n\n# Plot the Party ID variable\nlibrary(ggplot2)\n\n\n\nggplot(TEDS_2016, aes(PartyID)) + \n  geom_bar()\n\n\n\nggplot(TEDS_2016, aes(PartyID)) + \n  geom_bar(aes(y = (..count..)/sum(..count..))) + \n  scale_y_continuous(labels=scales::percent) +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\nggplot(TEDS_2016, aes(PartyID)) + \n  geom_bar(aes(y = (..count..)/sum(..count..),fill=PartyID)) + \n  scale_y_continuous(labels=scales::percent) +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw()\n\n\n\nggplot(TEDS_2016, aes(PartyID)) + \n  geom_bar(aes(y = (..count..)/sum(..count..),fill=PartyID)) + \n  scale_y_continuous(labels=scales::percent) +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))\n\n\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n✔ purrr   1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nTEDS_2016 %>% \n  count(PartyID) %>% \n  mutate(perc = n / nrow(TEDS_2016)) -> T2\nggplot(T2, aes(x = reorder(PartyID, -perc),y = perc,fill=PartyID)) + \n  geom_bar(stat = \"identity\") +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))\n\n\n\nTEDS_2016$Tondu<-factor(TEDS_2016$Tondu,labels=c(\"Unification now\",\"Status quo, unif. in >future\",\"Status quo, decide later\",\"Status quo forever\", \"Status quo, indep. in future\", \"Independence now\",\"No >response\"))\nlibrary(descr)\nfreq(TEDS_2016$Tondu)\n\n\n\n\nTEDS_2016$Tondu \n                             Frequency Percent\nUnification now                     27   1.598\nStatus quo, unif. in >future       180  10.651\nStatus quo, decide later           546  32.308\nStatus quo forever                 328  19.408\nStatus quo, indep. in future       380  22.485\nIndependence now                   108   6.391\nNo >response                       121   7.160\nTotal                             1690 100.000\n\nlibrary(ggplot2)\nggplot(TEDS_2016, aes(Tondu)) + \n  geom_bar()\n\n\n\nggplot(TEDS_2016, aes(Tondu)) + \n  geom_bar(aes(y = (..count..)/sum(..count..))) + \n  scale_y_continuous(labels=scales::percent) +\n  ylab(\"Count\") + \n  xlab(\"Tondu\")\n\n\n\nlibrary(tidyverse)\nTEDS_2016 %>% \n  count(Tondu) %>% \n  mutate(perc = n / nrow(TEDS_2016)) -> T2\nggplot(T2, aes(x = reorder(Tondu, -perc),y = perc,fill=Tondu)) + \n  geom_bar(stat = \"identity\") +\n  ylab(\"Count\") + \n  xlab(\"Tondu Variables\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))"
  },
  {
    "objectID": "assignment03.html",
    "href": "assignment03.html",
    "title": "assignment03",
    "section": "",
    "text": "library(haven)\nTEDS_2016 <- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n\nTEDS_2016$Tondu<-as.numeric(TEDS_2016$Tondu,labels=c(\"Unification now”, “Status quo, unif. in future”, “Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\", \"Independence now”, “No response\"))\n\n\nattach(TEDS_2016)\n\nmodl <- lm(Tondu ~ female + DPP + age + income+ edu + Taiwanese + Econ_worse)\n\nsummary (modl)\n\n\nCall:\nlm(formula = Tondu ~ female + DPP + age + income + edu + Taiwanese + \n    Econ_worse)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0523 -1.1411 -0.2287  0.7986  6.1354 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.204138   0.274188  15.333  < 2e-16 ***\nfemale       0.368598   0.082680   4.458 8.82e-06 ***\nDPP          0.121223   0.091598   1.323    0.186    \nage          0.001668   0.003143   0.531    0.596    \nincome      -0.025558   0.015862  -1.611    0.107    \nedu         -0.181096   0.036824  -4.918 9.61e-07 ***\nTaiwanese    0.768943   0.091437   8.410  < 2e-16 ***\nEcon_worse  -0.248545   0.083640  -2.972    0.003 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.667 on 1672 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.1074,    Adjusted R-squared:  0.1036 \nF-statistic: 28.73 on 7 and 1672 DF,  p-value: < 2.2e-16\n\nsubset <- subset(TEDS_2016, select = c(Tondu, female, DPP, age, income, edu, Taiwanese, Econ_worse))\n\nlibrary(ggplot2)\n\nggplot(subset, aes(x = Age, y = Tondu)) +\n\n  geom_point() +\n\n  geom_smooth(method = \"lm\")\n\nDon't know how to automatically pick scale for object of type\n<haven_labelled/vctrs_vctr/double>. Defaulting to continuous.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(subset, aes(x = edu, y = Tondu)) +\n\n  geom_point() +\n\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 10 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 10 rows containing missing values (`geom_point()`).\n\n\n\n\nggplot(subset, aes(x = income, y = Tondu)) +\n\n  geom_point() +\n\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe problem here is that the dependent variable has 7 categories. This regression analysis is meant for only one dependent variable which is why the graphs look like such. A better multivariate analysis method could make more sense of this."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nabil Salehiyan",
    "section": "",
    "text": "Graduate student studying machine learning, artificial intelligence, and neuroscience at The University of Texas at Dallas.\n\n\nMaster of Science date of completion: May, 2023.\nUndergraduate studies revolved around neuroscience, psychology, and computer science. After completion of B.S. in 2021, I began my masters at UT Dallas in order to combine my interests of data, ML/AI, and neuroscience. This website includes recent personal and academic projects, assignments with code, lab work, analytical methods used in multivariate analysis for data science, contact information, resume and more to come."
  },
  {
    "objectID": "projectproposal.html",
    "href": "projectproposal.html",
    "title": "projectproposal",
    "section": "",
    "text": "We want to analyze San Francisco 911 calls for service to determine if there is a discrepancy between priority given to neighborhoods based on median household income, despite an important emergency."
  },
  {
    "objectID": "projectproposal.html#data",
    "href": "projectproposal.html#data",
    "title": "projectproposal",
    "section": "Data:",
    "text": "Data:\nWe will be using the dataset “Law Enforcement Dispatched Calls for Service Real Time” found at data.gov. This is a continuously updating dataset from the City of San Francisco which began on December 3rd, 2021. To obtain demographic and income data for specific San Francisco neighborhoods, we will be using city-data.com from 2019."
  },
  {
    "objectID": "projectproposal.html#research-statement",
    "href": "projectproposal.html#research-statement",
    "title": "projectproposal",
    "section": "Research Statement:",
    "text": "Research Statement:\nAmerica has an apparent problem with its criminal justice system. Our law enforcement has marginalized certain groups of people based on socioeconomic status not just regarding proper situation handling but how fast the first responders show up to the scene. The question we want to answer using 911 emergency data from San Francisco is if there is a pattern or trend surrounding these calls and their responses. Does socioeconomic status determine the speed at which first responders show up at the scene? Being able to answer these questions will shed further light on the disparities that marginalized groups face in America, in hopes to find better solutions. The bias that officers have is not as inherent as people perceive it to be, research rather shows that these biases are more implicit and not self-actualized by law enforcement. If we are able to show this bias through knowledge-mining methods, these analyses could help bring more attention to the problems faced within our justice system."
  },
  {
    "objectID": "projectproposal.html#method",
    "href": "projectproposal.html#method",
    "title": "projectproposal",
    "section": "Method:",
    "text": "Method:\nWe will use a supervised logistic regression method with our output variable being onscene_datetime. Our input variables will be received_datetime, dispatch_datetime,enroute_datetime, intersection_name, analysis_neighborhood, and police_district. We will also add input variables of median household income and majority race for each neighborhood. The data from data.gov consists of recorded logs of these 911 calls from San Francisco. The goal of the knowledge mining method will be to extract the hidden pattern behind the data to see if there is a correlation between these variables."
  },
  {
    "objectID": "projectproposal.html#references",
    "href": "projectproposal.html#references",
    "title": "projectproposal",
    "section": "References:",
    "text": "References:\nCity of San Francisco. 2021-2023. Law Enforcement Dispatched Calls for Service Real Time. https://catalog.data.gov/dataset/law-enforcement-dispatched-calls-for-service-real-time"
  }
]